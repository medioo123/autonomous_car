{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Importation des packages</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'fonctionsUtiles'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0287ee226bf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfonctionsUtiles\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfonctionsUtiles\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marchitecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'fonctionsUtiles'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import cv2\n",
    "\n",
    "from fonctionsUtiles import functions as f\n",
    "from fonctionsUtiles import architecture\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.applications import NASNetLarge, InceptionV3\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Chargement de toutes les données</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Axionable data Loaded. Shape =  (26449, 90, 250, 3)\n",
      "Ironcar new track chicane Loaded. Shape =  (1519, 90, 250, 3)\n",
      "Ironcar old track data Loaded. Shape =  (16028, 90, 250, 3)\n"
     ]
    }
   ],
   "source": [
    "# Get training data from the Axionable track\n",
    "X_axio = np.load('../Datasets/axionable_data/X_train_axio.npy')\n",
    "Y_axio = np.load('../Datasets/axionable_data/Y_train_axio.npy')\n",
    "print('Axionable data Loaded. Shape = ', np.shape(X_axio))\n",
    "\n",
    "# Get training data from IronCar track\n",
    "# New track - Double chicane\n",
    "X_chicane = np.load('../Datasets/ironcar_data/new_track/x_chicane.npy')\n",
    "Y_chicane = np.load('../Datasets/ironcar_data/new_track/y_chicane.npy')\n",
    "print('Ironcar new track chicane Loaded. Shape = ', np.shape(X_chicane))\n",
    "\n",
    "# Old track - Balanced dataset\n",
    "X_iron = np.load('../Datasets/ironcar_data/old_track/balanced_iron_X.npy')\n",
    "Y_iron = np.load('../Datasets/ironcar_data/old_track/balanced_iron_Y.npy')\n",
    "print('Ironcar old track data Loaded. Shape = ', np.shape(X_iron))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Resizing des données</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Axionable data new size. Shape =  (26449, 331, 331, 3)\n",
      "Ironcar new track chicane new size. Shape =  (1519, 331, 331, 3)\n",
      "Ironcar old track data new size. Shape =  (16028, 331, 331, 3)\n"
     ]
    }
   ],
   "source": [
    "def resizing(array):\n",
    "    masterX = list()\n",
    "    for arr in array:\n",
    "        masterX.append(cv2.resize(arr, (331,331)))\n",
    "    return np.array(masterX)\n",
    "\n",
    "X_axio_resized = resizing(X_axio)\n",
    "X_chicane_resized = resizing(X_chicane)\n",
    "X_iron_resized = resizing(X_iron)\n",
    "\n",
    "print('Axionable data new size. Shape = ', np.shape(X_axio_resized))\n",
    "print('Ironcar new track chicane new size. Shape = ', np.shape(X_chicane_resized))\n",
    "print('Ironcar old track data new size. Shape = ', np.shape(X_iron_resized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Séparation des données en Train, Test et Validation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire qui stock quelques variables importantes\n",
    "args = {\"augmentation\": True,\n",
    "        \"train_split\": 0.8,\n",
    "        \"val_split\":0.2,\n",
    "        \"test_split\":0.2,\n",
    "        \"batch_size\": 200,\n",
    "        \"epochs\": 12,\n",
    "        \"early_stop\":True,\n",
    "        \"patience\":2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data loaded and concatenated. Shape =  (43996, 331, 331, 3)\n"
     ]
    }
   ],
   "source": [
    "# Concatenation de toutes les données chargées\n",
    "X = np.concatenate((X_axio_resized, X_chicane_resized, X_iron_resized))\n",
    "Y = np.concatenate((Y_axio, Y_chicane, Y_iron))\n",
    "print('All data loaded and concatenated. Shape = ', np.shape(X))\n",
    "\n",
    "# Suppression des variables tmp qui contiennent les images\n",
    "del X_axio, Y_axio, X_chicane, Y_chicane, X_iron, Y_iron, X_axio_resized, X_chicane_resized, X_iron_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réparatition du jeu de données en train test 80 - 20 %\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Répartition du jeu d'entrainement en jeu d'entrainement et en jeu de validation\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All training data loaded and augmented. Shape =  (28156, 331, 331, 3)\n",
      "All validation data loaded and augmented. Shape =  (7040, 331, 331, 3)\n",
      "All testing data loaded and augmented. Shape =  (8800, 331, 331, 3)\n"
     ]
    }
   ],
   "source": [
    "print('All training data loaded and augmented. Shape = ', np.shape(X_train))\n",
    "print('All validation data loaded and augmented. Shape = ', np.shape(X_val))\n",
    "print('All testing data loaded and augmented. Shape = ', np.shape(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des variables tmp qui contiennent les images\n",
    "del X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Augmentation des données</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 24/7039 [00:00<00:29, 239.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting data... Wait...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7039/7039 [00:16<00:00, 437.60it/s]\n",
      "100%|██████████| 7039/7039 [00:12<00:00, 552.43it/s]\n",
      "100%|██████████| 7039/7039 [00:00<00:00, 341326.36it/s]\n",
      "100%|██████████| 7039/7039 [00:19<00:00, 365.40it/s]\n",
      "100%|██████████| 7039/7039 [00:34<00:00, 203.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Axionable data after augmentation. Shape =  (63351, 331, 331, 3)\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation of the dataset / Adjust the proportion of each transformation you want to apply.\n",
    "if args['augmentation']:\n",
    "    print('Augmenting data... Wait...')\n",
    "    # Data augmentation 25% of random brightness.\n",
    "    X_bright, Y_bright = f.generate_brightness(X_train, Y_train, proportion=0.25)\n",
    "    # Data augmentation 25% of night effect.\n",
    "    X_night, Y_night = f.generate_night_effect(X_train, Y_train, proportion=0.25)\n",
    "    # Data augmentation 25% of horizontal flipping.\n",
    "    X_flip, Y_flip = f.generate_horizontal_flip(X_train, Y_train, proportion=0.25)\n",
    "    # Data augmentation 25% of random shadows.\n",
    "    X_shadow, Y_shadow = f.generate_random_shadows(X_train, Y_train, proportion=0.25)\n",
    "    # Data augmentation 25% of chained tranformations (bright + shadows + flip).\n",
    "    X_chain, Y_chain = f.generate_chained_transformations(X_train, Y_train, proportion=0.25)\n",
    "\n",
    "    # Concatenating Axionable dataset with the transformations.\n",
    "    X_train = np.concatenate((X_train, X_bright, X_night,\n",
    "                                X_flip, X_shadow, X_chain))\n",
    "\n",
    "    Y_train = np.concatenate((Y_train, Y_bright, Y_night, \n",
    "                                Y_flip, Y_shadow, Y_chain)).astype('float32')\n",
    "\n",
    "    print('Axionable data after augmentation. Shape = ', np.shape(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des variables tmp qui contiennent les images\n",
    "del X_bright, Y_bright, X_night, Y_night, X_flip, Y_flip, X_shadow, Y_shadow, X_chain, Y_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Construction du modèle</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6da8e74b0b2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'linear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# On indique que les couches avant la couche de classification ne doivent pas être entrainées\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Model' is not defined"
     ]
    }
   ],
   "source": [
    "# Nombre de classes en output du modèle. Ici : hard left, left, forward, right et hard right\n",
    "num_classes = 5\n",
    "\n",
    "# Initialisation du modèle : on télécharge le modèle MobilNetV2 et on rajoute la dernière couche\n",
    "# de classification propre à notre problème\n",
    "# my_new_model = Sequential()\n",
    "# my_new_model.add(InceptionV3(include_top=True, pooling='max', weights=\"imagenet\"))\n",
    "# my_new_model.add(Dense(128, activation='relu'))\n",
    "# my_new_model.add(Dropout(0.2))\n",
    "# my_new_model.add(Dense(128, activation='relu'))\n",
    "# my_new_model.add(Dropout(0.2))\n",
    "# my_new_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "l = Dense(128, activation = 'relu')(base_model.output)\n",
    "l = Dropout(0.2)(l)\n",
    "l = Dense(64, activation = 'relu')(l)\n",
    "l = Dropout(0.2)(l)\n",
    "l = Flatten()(l)\n",
    "predictions = Dense(1, activation = 'linear')(l)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# On indique que les couches avant la couche de classification ne doivent pas être entrainées\n",
    "# my_new_model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inception_v3 (Model)         (None, 1000)              23851784  \n",
      "=================================================================\n",
      "Total params: 23,851,784\n",
      "Trainable params: 0\n",
      "Non-trainable params: 23,851,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# On vérfie la structure du modèle\n",
    "my_new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On définit la fonction d'optimisation du modèle\n",
    "my_new_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0617 17:30:31.790341 139796909606656 training.py:618] The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63351 samples, validate on 7040 samples\n",
      "Epoch 1/12\n",
      "  400/63351 [..............................] - ETA: 5:08:55 - loss: 8.2947 - acc: 0.2800"
     ]
    }
   ],
   "source": [
    "# Paramètre de réduction du learning rate si l'accuracy en validation diminue\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', \n",
    "                              factor=0.2,\n",
    "                              patience=5, \n",
    "                              min_lr=0.001)\n",
    "\n",
    "hist = my_new_model.fit(\n",
    "                X_train, \n",
    "                Y_train,\n",
    "                nb_epoch= args['epochs'],\n",
    "                batch_size=args['batch_size'], \n",
    "                verbose=1, \n",
    "                validation_data=(X_val, Y_val),\n",
    "                shuffle=True, \n",
    "                callbacks = [reduce_lr])\n",
    "\n",
    "my_new_model.save('TransferNASNetLarge_12Epochs_doubledenselayer.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
